#!/usr/bin/env ruby

require 'optparse'
require 'readline'
require_relative '../lib/llms'

# Never change this!
TEMPERATURE = 0.0

options = {
  stream: true,
  system_prompt: "You are a helpful assistant",
  model: "claude-3-5-sonnet-latest",
  max_tokens: 2048,
  thinking: false,
  thinking_max_tokens: 1024
}

OptionParser.new do |opts|
  opts.banner = "Usage: run-executor [options] [PROMPT]"

  opts.on("--no-stream", "Disable streaming output") do
    options[:stream] = false
  end

  opts.on("--system PROMPT", "Set custom system prompt") do |prompt|
    options[:system_prompt] = prompt
  end

  opts.on("-m", "--model MODEL", "Specify model name") do |model|
    options[:model] = model
  end

  opts.on("--max-tokens N", Integer, "Set max tokens (default: #{options[:max_tokens]})") do |n|
    options[:max_tokens] = n
  end

  opts.on("-t", "--thinking", "Enable thinking mode") do
    options[:thinking] = true
  end

  opts.on("--thinking-max-tokens N", Integer, "Set max tokens for thinking mode (default: #{options[:thinking_max_tokens]})") do |n|
    options[:thinking_max_tokens] = n
  end
end.parse!

executor = LLMs::Executors.instance(
  model_name: options[:model],
  temperature: TEMPERATURE,
  max_tokens: options[:max_tokens],
  thinking: options[:thinking],
  thinking_max_tokens: options[:thinking_max_tokens]
)

def run_chat(executor, options)
  puts executor.model_name
  conversation = LLMs::Conversation.new
  conversation.set_system_message(options[:system_prompt])

  # Set up command history
  Readline.completion_append_character = " "
  Readline.completion_proc = proc { |s| [] }

  loop do
    prompt = Readline.readline("> ", true)
    break if prompt.nil? || prompt.empty? || prompt == "exit"

    conversation.add_user_message(prompt)

    if options[:stream]
      executor.execute_conversation(conversation) { |chunk| print chunk }
      puts
    else
      response = executor.execute_conversation(conversation)
      puts response.text
    end

    if error = executor.last_error
      $stderr.puts "Error: #{error.inspect}"
    end

    usage = executor.last_usage_data
    $stderr.puts "Usage: #{usage}"

    conversation.add_assistant_message(executor.last_received_message)
  end
end

def run_prompt(executor, prompt, options)
  puts executor.model_name
  if options[:stream]
    #executor.execute_prompt(prompt.strip, system_prompt: options[:system_prompt]) { |chunk| print chunk }
    executor.execute_prompt(prompt.strip) { |chunk| print chunk }
    puts
  else
    response = executor.execute_prompt(prompt.strip)
    puts response
  end

  if error = executor.last_error
    $stderr.puts "Error: #{error.inspect}"
  end

  usage = executor.last_usage_data
  $stderr.puts "Usage: #{usage}"
end

if ARGV.empty?
  run_chat(executor, options)
else
  run_prompt(executor, ARGV.join(' '), options)
end
