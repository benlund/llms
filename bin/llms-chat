#!/usr/bin/env ruby

require 'optparse'
require 'readline'
require_relative '../lib/llms'

# Never change this!
TEMPERATURE = 0.0

options = {
  stream: true,
  system_prompt: "You are a helpful assistant",
  model: "claude-3-5-sonnet-latest",
  max_tokens: 2048,
  thinking: false,
  thinking_max_tokens: 1024
}

OptionParser.new do |opts|
  opts.banner = "Usage: run-executor [options] [PROMPT]"

  opts.on("--no-stream", "Disable streaming output") do
    options[:stream] = false
  end

  opts.on("--system PROMPT", "Set custom system prompt") do |prompt|
    options[:system_prompt] = prompt
  end

  opts.on("-m", "--model MODEL", "Specify model name") do |model|
    options[:model] = model
  end

  opts.on("--max-tokens N", Integer, "Set max tokens (default: #{options[:max_tokens]})") do |n|
    options[:max_tokens] = n
  end

  opts.on("-t", "--thinking", "Enable thinking mode") do
    options[:thinking] = true
  end

  opts.on("--thinking-max-tokens N", Integer, "Set max tokens for thinking mode (default: #{options[:thinking_max_tokens]})") do |n|
    options[:thinking_max_tokens] = n
  end

  opts.on("-l", "--list-models", "List available models") do
    options[:list_models] = true
  end

  opts.on("-q", "--quiet", "Don't print connetion or usage info") do
    options[:quiet] = true
  end

end.parse!

def run_chat(executor, options)
  conversation = LLMs::Conversation.new
  conversation.set_system_message(options[:system_prompt])

  # Set up command history
  Readline.completion_append_character = " "
  Readline.completion_proc = proc { |s| [] }

  loop do
    prompt = Readline.readline("> ", true)
    break if prompt.nil? || prompt.empty? || prompt == "exit"

    conversation.add_user_message(prompt)

    if options[:stream]
      executor.execute_conversation(conversation) { |chunk| print chunk }
      puts
    else
      response = executor.execute_conversation(conversation)
      puts response.text
    end

    if error = executor.last_error
      $stderr.puts "Error: #{error.inspect}"
    end

    unless options[:quiet]
      usage = executor.last_usage_data
      puts "Usage: #{usage}"
    end

    conversation.add_assistant_message(executor.last_received_message)
  end
end

def run_prompt(executor, prompt, options)
  if options[:stream]
    #executor.execute_prompt(prompt.strip, system_prompt: options[:system_prompt]) { |chunk| print chunk }
    executor.execute_prompt(prompt.strip) { |chunk| print chunk }
    puts
  else
    response = executor.execute_prompt(prompt.strip)
    puts response
  end

  if error = executor.last_error
    $stderr.puts "Error: #{error.inspect}"
  end

  unless options[:quiet]
    usage = executor.last_usage_data
    puts "Usage: #{usage}"
  end
end

def list_models
  puts LLMs::Models.list_model_names
end

if options[:list_models]
  list_models
else
  executor = LLMs::Executors.instance(
    model_name: options[:model],
    temperature: TEMPERATURE,
    max_tokens: options[:max_tokens],
    thinking: options[:thinking],
    thinking_max_tokens: options[:thinking_max_tokens]
  )
  unless options[:quiet]
    puts "Connected to: #{executor.model_name} (#{executor.class.name})"
  end
  if ARGV.empty?
    run_chat(executor, options)
  else
    run_prompt(executor, ARGV.join(' '), options)
  end
end
